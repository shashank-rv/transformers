{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport sys","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\ntokenizer =BertTokenizer.from_pretrained('bert-base-multilingual-cased')","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ea5554a765440d84b0ccb85e9fea4b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train1[1:10000]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = train2.comment_text.values\nlabels = train2.toxic.values","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid2 = valid[1:1000]","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_valid = valid2.comment_text.values\nlabels_valid = valid2.toxic.values","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = []\nfor sent in tqdm(sentences):\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 512,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = False) # Construct attn. masks.\n                        #return_tensors = 'tf')  \n    input_ids.append(encoded_dict['input_ids'])","execution_count":10,"outputs":[{"output_type":"stream","text":"100%|██████████| 9999/9999 [00:30<00:00, 329.51it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids_valid = []\nfor sent in tqdm(sentences_valid):\n    encoded_dict = tokenizer.encode_plus(\n                        sent,                      # Sentence to encode.\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 512,           # Pad & truncate all sentences.\n                        pad_to_max_length = True,\n                        return_attention_mask = False) # Construct attn. masks.\n                        #return_tensors = 'tf')  \n    input_ids_valid.append(encoded_dict['input_ids'])","execution_count":11,"outputs":[{"output_type":"stream","text":"100%|██████████| 999/999 [00:02<00:00, 352.52it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((input_ids,labels)).shuffle(10).batch(64).repeat()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_valid = tf.data.Dataset.from_tensor_slices((input_ids_valid,labels_valid)).batch(64)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create strategy from tpu\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    bert_model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n    input_word_ids = Input(shape=(512,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = bert_model(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    pre_out=Dense(192,activation = \"relu\")(cls_token)\n    out = Dense(1, activation='sigmoid')(pre_out)\n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])","execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2012a9faebb04df29397cc0e6186c8a2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1083389348.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb3222556134f0daa0381cf20df9197"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":17,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 512)]             0         \n_________________________________________________________________\ntf_bert_model (TFBertModel)  ((None, 512, 768), (None, 177853440 \n_________________________________________________________________\ntf_op_layer_strided_slice (T [(None, 768)]             0         \n_________________________________________________________________\ndense (Dense)                (None, 192)               147648    \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 193       \n=================================================================\nTotal params: 178,001,281\nTrainable params: 178,001,281\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = model.fit(dataset,steps_per_epoch=200,epochs=10,validation_data=dataset_valid,verbose=1)","execution_count":21,"outputs":[{"output_type":"stream","text":"Train for 200 steps, validate for 16 steps\nEpoch 1/10\n200/200 [==============================] - 185s 924ms/step - loss: 0.2009 - accuracy: 0.9259 - val_loss: 0.3700 - val_accuracy: 0.8338\nEpoch 2/10\n200/200 [==============================] - 49s 245ms/step - loss: 0.1093 - accuracy: 0.9573 - val_loss: 0.4205 - val_accuracy: 0.8318\nEpoch 3/10\n200/200 [==============================] - 49s 244ms/step - loss: 0.0658 - accuracy: 0.9754 - val_loss: 0.5288 - val_accuracy: 0.8298\nEpoch 4/10\n200/200 [==============================] - 49s 243ms/step - loss: 0.0488 - accuracy: 0.9819 - val_loss: 0.4046 - val_accuracy: 0.8238\nEpoch 5/10\n200/200 [==============================] - 49s 243ms/step - loss: 0.0405 - accuracy: 0.9848 - val_loss: 0.4364 - val_accuracy: 0.8408\nEpoch 6/10\n200/200 [==============================] - 49s 245ms/step - loss: 0.0300 - accuracy: 0.9879 - val_loss: 0.5143 - val_accuracy: 0.8378\nEpoch 7/10\n200/200 [==============================] - 49s 243ms/step - loss: 0.0165 - accuracy: 0.9944 - val_loss: 0.6124 - val_accuracy: 0.8408\nEpoch 8/10\n200/200 [==============================] - 49s 244ms/step - loss: 0.0130 - accuracy: 0.9950 - val_loss: 0.7377 - val_accuracy: 0.8258\nEpoch 9/10\n200/200 [==============================] - 49s 244ms/step - loss: 0.0143 - accuracy: 0.9950 - val_loss: 0.5952 - val_accuracy: 0.8338\nEpoch 10/10\n200/200 [==============================] - 49s 244ms/step - loss: 0.0105 - accuracy: 0.9960 - val_loss: 0.5705 - val_accuracy: 0.8438\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history.history","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f30af9c1400>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}